{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script that replaces words with the WordNet synset ID**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "#import data set \n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount = True)\n",
    "import csv\n",
    "path = \"/content/drive/MyDrive/Colab Notebooks/sample100abstracts.tsv\"\n",
    "from nltk.corpus import wordnet as wn # Import Wordnet\n",
    "from collections import Counter  # Import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ten first lines\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "tsv_file = open(path)\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "i = 0\n",
    "for row in read_tsv:\n",
    "  print(row)\n",
    "  i+=1\n",
    "  if i>10:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Arragnement(): takes a list and returns list of sorted unique variables according to frequency \n",
    "              urgument: 'list' a list of data set \n",
    "              return: 'unique' a list of sorted variables in order of frequency  \n",
    "              i.e the first element is a unique word which as a high frequency\n",
    "\"\"\"\n",
    "def arrangement(list):\n",
    "  result = sorted(list, key = list.count, reverse = True) # sorting on basis of frequency of elements\n",
    "  used = set()\n",
    "  unique = [x for x in result if x not in used and (used.add(x) or True)]  #arrange according to unique characters\n",
    "  return(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "synonym(): Takes a word and prints its synonyms in form of a list (synset) using wordnet \n",
    "          @urgument: 'word' a string or any variable part of the dataset  \n",
    "          @return: 'synonyms' a list of synonyms of the words \n",
    "\"\"\"\n",
    "def synonym(word):\n",
    "  synonyms = []\n",
    "  for syn in wn.synsets(word):\n",
    "    for l in syn.lemmas():\n",
    "      synonyms.append(l.name())\n",
    "  return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "giveKey(): Takes a word and dictionary and returns the key of the word in the dictionary  \n",
    "          @urgument: 'word' a string or any variable part of the dataset to be replaced with the dictionary key if it is not a key itself\n",
    "                     \"doctList\" a dictionary created with the whole dataset   \n",
    "          @return: 'Key_lis[x]' a string of a unique key of the word in the dictionary \n",
    "\"\"\"\n",
    "def giveKey(word, dictList):\n",
    "  key_list = []\n",
    "  val_list = []\n",
    "  key_list.extend(dictList.keys())\n",
    "  val_list.extend(dictList.values())\n",
    "  for x in range(len(val_list)):\n",
    "    for j in range(len(val_list[x])):\n",
    "      if(val_list[x][j] == word):\n",
    "        return(key_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dictCreate(): Creates a dictionary from the whole data set, they keys are in order of their frequency words and the values are synonyms of keys form synset   \n",
    "          @urgument: 'unique' a list of unique variables from the wholed dataset in order of their frequency\n",
    "          @return: 'diction' a dictionary of all the variables in the dataset, the keys are the unique variables with high frequency, and values are key's synonym\n",
    "\"\"\"\n",
    "def dictCreate(unique):\n",
    "    valueSynm = [] #create list of values\n",
    "    keys = [unique[0]] #enter the first word from the list and its synonym\n",
    "    values = [synonym(unique[0])]\n",
    "    diction = dict(zip(keys, values)) \n",
    "    valueSynm.extend(synonym(unique[0]))\n",
    "    for x in range(1,len(unique)):\n",
    "      if(unique[x] in valueSynm): #check if the variable is the unique list is part of values (synonyms) of already existing dictionary, and skip that word \n",
    "        continue\n",
    "      else:\n",
    "        valueSynm.extend(synonym(unique[x])) \n",
    "        diction[unique[x]] = synonym(unique[x]) # adding a dicitonary \n",
    "    return diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file = open(path)\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "temp = []\n",
    "for row in read_tsv:\n",
    "  corpus_raw = row\n",
    "  raw_sentences = row[2].split('.')\n",
    "  sentences = []\n",
    "  for sentence in raw_sentences:\n",
    "    temp.extend(sentence.split())\n",
    "dictionary = dictCreate(arrangement(temp)) \n",
    "tsv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file = open(path)\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "for row in read_tsv:\n",
    "  corpus_raw = row\n",
    "  raw_sentences = row[2].split('.')\n",
    "  sentences = []\n",
    "  for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "    for i in range(len(sentences)):\n",
    "      if sentences[0][i] in dictionary:\n",
    "        continue\n",
    "      else:\n",
    "        x = giveKey(sentences[i], dictionary)\n",
    "        sentences[i] = x\n",
    "  print(sentences[0])\n",
    "tsv_file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
