{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python script that replaces words with the WordNet synset ID**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "#nltk.download(\"wordnet\")\n",
    "#import data set \n",
    "\n",
    "marea_file = \"data/sample100abstracts.tsv\"\n",
    "from nltk.corpus import wordnet as wn # Import Wordnet\n",
    "from collections import Counter  # Import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sample100abstracts.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/d5v9f8854nj1wp1z9gbw_d1x2smqd5/T/ipykernel_5397/940098117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarea_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mread_tsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sample100abstracts.tsv'"
     ]
    }
   ],
   "source": [
    "# Read ten first lines\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "tsv_file = open(marea_file)\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "i = 0\n",
    "for row in read_tsv:\n",
    "  print(row)\n",
    "  i+=1\n",
    "  if i>10:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Arragnement(): takes a list and returns list of sorted unique variables according to frequency \n",
    "              urgument: 'list' a list of data set \n",
    "              return: 'unique' a list of sorted variables in order of frequency  \n",
    "              i.e the first element is a unique word which as a high frequency\n",
    "\"\"\"\n",
    "def arrangement(list):\n",
    "  result = sorted(list, key = list.count, reverse = True) # sorting on basis of frequency of elements\n",
    "  used = set()\n",
    "  unique = [x for x in result if x not in used and (used.add(x) or True)]  #arrange according to unique characters\n",
    "  return(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "synonym(): Takes a word and prints its synonyms in form of a list (synset) using wordnet \n",
    "          @urgument: 'word' a string or any variable part of the dataset  \n",
    "          @return: 'synonyms' a list of synonyms of the words \n",
    "\"\"\"\n",
    "def synonym(word):\n",
    "  synonyms = []\n",
    "  for syn in wn.synsets(word):\n",
    "    for l in syn.lemmas():\n",
    "      synonyms.append(l.name())\n",
    "  return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "giveKey(): Takes a word and dictionary and returns the key of the word in the dictionary  \n",
    "          @urgument: 'word' a string or any variable part of the dataset to be replaced with the dictionary key if it is not a key itself\n",
    "                     \"doctList\" a dictionary created with the whole dataset   \n",
    "          @return: 'Key_lis[x]' a string of a unique key of the word in the dictionary \n",
    "\"\"\"\n",
    "def giveKey(word, dictList):\n",
    "  key_list = []\n",
    "  val_list = []\n",
    "  key_list.extend(dictList.keys())\n",
    "  val_list.extend(dictList.values())\n",
    "  for x in range(len(val_list)):\n",
    "    for j in range(len(val_list[x])):\n",
    "      if(val_list[x][j] == word):\n",
    "        return(key_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dictCreate(): Creates a dictionary from the whole data set, they keys are in order of their frequency words and the values are synonyms of keys form synset   \n",
    "          @urgument: 'unique' a list of unique variables from the wholed dataset in order of their frequency\n",
    "          @return: 'diction' a dictionary of all the variables in the dataset, the keys are the unique variables with high frequency, and values are key's synonym\n",
    "\"\"\"\n",
    "def dictCreate(unique):\n",
    "    valueSynm = [] #create list of values\n",
    "    keys = [unique[0]] #enter the first word from the list and its synonym\n",
    "    values = [synonym(unique[0])]\n",
    "    diction = dict(zip(keys, values)) \n",
    "    valueSynm.extend(synonym(unique[0]))\n",
    "    for x in range(1,len(unique)):\n",
    "      if(unique[x] in valueSynm): #check if the variable is the unique list is part of values (synonyms) of already existing dictionary, and skip that word \n",
    "        continue\n",
    "      else:\n",
    "        valueSynm.extend(synonym(unique[x])) \n",
    "        diction[unique[x]] = synonym(unique[x]) # adding a dicitonary \n",
    "    return diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sample100abstracts.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rp/d5v9f8854nj1wp1z9gbw_d1x2smqd5/T/ipykernel_5397/1022183606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarea_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mread_tsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_tsv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mcorpus_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sample100abstracts.tsv'"
     ]
    }
   ],
   "source": [
    "tsv_file = open(marea_file)\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "temp = []\n",
    "for row in read_tsv:\n",
    "  corpus_raw = row\n",
    "  raw_sentences = row[2].split('.')\n",
    "  sentences = []\n",
    "  for sentence in raw_sentences:\n",
    "    temp.extend(sentence.split())\n",
    "dictionary = dictCreate(arrangement(temp)) \n",
    "tsv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file = open(marea_file)\n",
    "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\n",
    "for row in read_tsv:\n",
    "  corpus_raw = row\n",
    "  raw_sentences = row[2].split('.')\n",
    "  sentences = []\n",
    "  for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "    for i in range(len(sentences)):\n",
    "      if sentences[0][i] in dictionary:\n",
    "        continue\n",
    "      else:\n",
    "        x = giveKey(sentences[i], dictionary)\n",
    "        sentences[i] = x\n",
    "  print(sentences[0])\n",
    "tsv_file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
